\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\renewcommand{\rmdefault}{ppl} % rm
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}
\usepackage{caption} % <-- add this in your new preamble
\usepackage[svgnames]{xcolor}
\usepackage[most]{tcolorbox}

\newcommand{\innerp}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\norm}[2]{\lVert#1\rVert_{#2}}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,     
	urlcolor=cyan,
}

% --- SOLUTION BOX STYLE ---
\newtcolorbox{solutionbox}{
	enhanced,         % enables advanced features
	breakable,        % allows box to break across pages
	colback=white,    % background color
	colframe=black,   % slightly gray frame
	coltitle=white,     % title text color
	fonttitle=\bfseries,    % bold title
	title={\textbf{Solution}}, % box title
	rounded corners,    % smooth corners
	boxrule=0.8pt,        % border thickness
	%	attach boxed title to top left={xshift=0mm, yshift*=0mm},
	boxed title style={
		frame hidden,
		sharp corners
	},
	top=4pt, bottom=4pt, left=6pt, right=6pt, % padding inside box
	before skip=10pt, after skip=10pt,         % vertical spacing
}

% --- DOCUMENT START ---
\begin{document}

% --- TITLE ---
\hrule
\vspace{3mm}
\noindent 
{\sf IITM-CS5691 : Pattern Recognition and Machine Learning  \hfill Release Date: Oct 24, 2025}
\\
\noindent 
{\sf Assignment 3 \hfill Due Date : Nov 08, 2025, {\bf 9pm}}
%{\sf ~\hfill }
\vspace{3mm}
\hrule
\vspace{3mm}
\noindent{{\sf Roll No:EE25S009} }% put your ROLL NO AND NAME HERE

\noindent
{{\sf Collaborators (if any): }\hfill  {\sf Name: RITABRATA MANDAL}} %Names of the collaborators (if any).

\noindent
{{\sf References/sources (if any): 
}} %Reference/source materials, if any.

\vspace{3mm}
\hrule

% --- INSTRUCTIONS ---
\subsection*{General Instructions}
\begin{itemize}[leftmargin=*]
\item Use \LaTeX\ to write-up your solutions (in the solution blocks of the source \LaTeX\ file of this assignment), and submit the resulting {\bf single pdf file} at Gradescope by the due date. (Note: {\bf No late submissions} will be allowed, other than one-day late submission with 10\% penalty or four-day late submission with 30\% penalty! You can join Gradescope using the course entry code 6K4P43. {\bf Within Gradescope, clearly mark your answer to each question}, else we won't be able to grade it.)
\item For the programming question, please submit your code (rollno.ipynb file and rollno.py file in rollno.zip) directly in moodle, but provide your results/answers (including Jupyter notebook {\bf with output}) in the pdf file you upload to Gradescope.
\item Collaboration is encouraged, but all write-ups must be done individually and independently, and mention your collaborator(s) if any. Same rules apply for codes written for any programming assignments (i.e., write your own code; we will run plagiarism and AI detection checks on codes).
\item  If you have referred a book or any other online material for obtaining a solution, please cite the source. Again don't copy the source {\it as is} - you may use the source to understand the solution, but write-up the solution in your own words.   
\item For all the reasons explained in class, you cannot feed these questions into LLMs (Large Language Models like ChatGPT) and cannot use the LLMs' outputs to answer this assignment. Related to this, please also complete the self-declaration statement in the end of your answer sheet pdf. 
\item Please be advised that {\it the lesser your reliance on online materials or LLMs} for answering the questions, {\it the more your understanding} of the concepts will be and {\it the more prepared you will be for the course exams}.
\item Points will be awarded based on how clear, concise and rigorous your solutions are, and how correct your answer is. The weightage of this assignment is 11\% towards the overall course grade. 
\end{itemize}
\hrule
\newpage
%\vspace{5mm}

% --- QUESTIONS ---
\begin{enumerate}

    % --- QUESTION 1 [Logistic Regression: Theory and Practice]---
    \item \textbf{(8 points) [Logistic Regression: Theory and Practice]} \\

    \begin{enumerate}[label=(\alph*)]
        \item \textbf{(1 point)} Briefly explain why logistic regression is called "regression", despite it being a classification model.
        
        \begin{solutionbox}
        	Logistic regression is called ``regression'' because, it applies a logistic transformation to a liner function $w^{T}x+b$ to produce the class probability. As the underlying model estimates probabilities through a regression based approach, it is called logistic regression.
        \end{solutionbox}
        
        \item \textbf{(3 points)} Consider the following 2-dimensional classification dataset with 8 points:
        \[ X^T = \begin{bmatrix}
        -2 & -2 & -1 & 1 & 1 & 2 & 3 & 3 \\
        -1 & 2 & 1 & 2 & 1 & 3 & 3 & 2
        \end{bmatrix} \]
        \[ y^T = \begin{bmatrix}
        +1 & +1 & +1 & -1 & -1 & +1 & -1 & -1
        \end{bmatrix} \]
        Run \textbf{one iteration} of gradient descent for the logistic loss objective $L(w) = \sum_{i=1}^8 \log(1 + \exp(-y_i w^T x_i))$ by hand. Use an initial weight vector $w^{(0)} = [0, 0]^T$ and a step size $\eta = 1$. No bias term is required. Show your calculation for the gradient $\nabla L(w^{(0)})$ and the updated weight vector $w^{(1)}$.
        
        \begin{solutionbox}
        	The logistic loss objective function is
        	\begin{align*}
        		L(w)=\sum_{i=1}^{8}\log(1+\exp{(-y_{i}w^{T}x_{i})})
        	\end{align*}
        	The gradient of the loss function w.r.to $w$ 
        	\begin{align*}
        		\nabla_{w} L(w)&=\sum_{i=1}^{8}\frac{1}{1+\exp(-y_{i}w^{T}x_{i})}\cdot \exp(-y_{i}w^{T}x_{i})\cdot(-y_{i}x_{i})\\
        		&=-\sum_{i=1}^{8}\frac{y_{i}x_{i}}{1+\exp(y_{i}w^{T}x_{i})}
        	\end{align*}
        	Now, the gradient at the initial weight $w^{(0)}=\begin{bmatrix}0&0\end{bmatrix}^{T}$
        	\begin{align*}
        		\nabla_{w}L(w^{(0)})&=-\sum_{i=1}^{8}\frac{y_ix_i}{1+\exp(y_{i}{w^{(0)}}^{T}x_{i})}=-\frac{1}{2}\sum_{i=1}^{8}y_{i}x_{i}\\
        		&= -\frac{1}{2}\left(\begin{bmatrix} -2 \\ -1 \end{bmatrix} + \begin{bmatrix} -2 \\ 2 \end{bmatrix} + \begin{bmatrix} -1 \\ 1 \end{bmatrix} -\begin{bmatrix} 1 \\ 2 \end{bmatrix} -\begin{bmatrix} 1 \\ 1 \end{bmatrix} + \begin{bmatrix} 2 \\ 3 \end{bmatrix} -\begin{bmatrix} 3 \\ 3 \end{bmatrix} -\begin{bmatrix} 3 \\ 2 \end{bmatrix}\right) \\
        		&= \begin{bmatrix} 11/2 \\ 3/2 \end{bmatrix}=\begin{bmatrix}5.5\\1.5\end{bmatrix}
        	\end{align*}
        	By the gradient descend update rule
        	\begin{align*}
        		w^{(1)}&=w^{(0)}-\eta\nabla_{w}(w^{(0)})\\
        		&=\begin{bmatrix}0\\0\end{bmatrix}-\begin{bmatrix}5.5\\1.5\end{bmatrix}=\begin{bmatrix}-5.5\\-1.5\end{bmatrix}
        	\end{align*}
        \end{solutionbox}
    
        \item \textbf{(1 point)} Using the updated weight vector $w^{(1)}$ you just calculated, what is the equation of the decision boundary (where $P	(y=1|x) = 0.5$)?
        
        \begin{solutionbox}
        	The equation of the decision boundary
        	\begin{align*}
        		P(y=1\mid x) =0.5&=\sigma({w^{(1)}}^{T} x)\\
        		\Rightarrow0.5&=\frac{1}{1+\exp\left(-{w^{(1)}}^{T} x\right)}\\
        		\Rightarrow 2& = 1+\exp\left(-{w^{(1)}}^{T} x\right)\\
        		\Rightarrow {w^{(1)}}^{T}x&=0\Rightarrow \begin{bmatrix}-5.5 & -1.5\end{bmatrix}x=0
        	\end{align*}
        \end{solutionbox}
    
        \item \textbf{(1 point)} Using $w^{(1)}$, what is the probability $P(y=1|x)$ for a new test point $x_{new} = [2, 2]^T$? (Recall $P(y=1|x) = \sigma(w^T x)$, where $\sigma(z) = 1 / (1 + e^{-z})$).
        
        \begin{solutionbox}
        	The probability
        	\begin{align*}
        		P(y=1\mid x_{new})=\sigma\left({w^{(1)}}^{T}x_{new}\right)=\sigma\left(-14\right)=1/(1+e^{14})=8.315\times 10^{-7}\approx 0
        	\end{align*}
        \end{solutionbox}
    
        \item \textbf{(1 point)} If you were to continue training this model and found it was overfitting, you might add L2 regularization. Explain what effect adding strong L2 regularization (a large $\lambda$) would have on the magnitude of the final, converged weight vector $\hat{w}$.
        
        \begin{solutionbox}
        	If we add strong L2 regularization then the model minimizes the norm of the weight vector, leading to a smaller magnitude final weight vector $\hat{w},$ which in turn leads to underfitting of the data.
        \end{solutionbox}
    
    \end{enumerate}

    % --- QUESTION 2 [THE KERNEL TRICK AND ITS ALTERNATIVES]---
    \item \textbf{(8 points) [THE KERNEL TRICK AND ITS ALTERNATIVES]} \\
    Consider a 1D dataset that is not linearly separable:
    \begin{itemize}
        \item \textbf{Class +1:} $x = -2$, $x = 2$
        \item \textbf{Class -1:} $x = -1$, $x = 1$
    \end{itemize}
    We want to make this dataset linearly separable by mapping it to a higher-dimensional space using a feature map $\phi(x)$.
    
    \begin{enumerate}[label=(\alph*)]
        \item \textbf{(1 point)} In your own words, what is the primary purpose of the kernel trick in algorithms like SVM? Why is it useful?
        
        \begin{solutionbox}
        	As SVM is an algorithm that maximizes the linear decision boundary in the dataset, if the dataset is not linearly separable, then SVM will fail to perform classification. However, the kernel trick maps the datapoints to higher-order feature dimensions, where they can be linearly separable, allowing SVM to work. 
        \end{solutionbox}
        
        \item \textbf{(2 points)} Propose a simple feature mapping $\phi(x): \mathbb{R} \to \mathbb{R}^2$ that transforms the given 1D data into a 2D space where it becomes linearly separable. Plot or list the transformed coordinates of the four data points to show they are now separable.
        
        \begin{solutionbox}
        	Let the feature mapping be
        	\begin{align*}
        		\phi(x)=\begin{bmatrix}x\\x^{2}\end{bmatrix}
        	\end{align*}
        	Figure \ref{fig:feature-map} shows the full feature mapping and decision boundary. 
        	\begin{center}
        		\includegraphics[width=\textwidth]{figure_2b.pdf}
        		\captionof{figure}{Feature mapping visualization}
        		\label{fig:feature-map}
        	\end{center}
        \end{solutionbox}
        
        \item \textbf{(2 points)} For the feature mapping $\phi(x)$ you defined in part (b), derive the corresponding kernel function $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$. Show your work.
        
        \begin{solutionbox}
        	For the feature map 
        	\begin{align*}
        		\phi(x)=\begin{bmatrix}x\\x^{2}\end{bmatrix}
        	\end{align*}
        	The kernel function 
        	\begin{align*}
        		K(x_i,x_{j})&=\phi(x_{i})^{T}\phi(x_{j})=\begin{bmatrix}x_{i}&x_{i}^{2}\end{bmatrix}\begin{bmatrix}x_{j}\\x_{j}^{2}\end{bmatrix}=x_{i}x_{j}+x_{i}^{2}x_{j}^{2}
        	\end{align*}
        	Since this kernel is explicitly defined as an inner product in the feature space, it is automatically a valid kernel.
        \end{solutionbox}
    
        \item \textbf{(3 points)} Now, suppose you did not want to use a kernel (i.e., you stay in the original 1D space and build a linear SVM model). Could this 1D dataset be solved using a \textbf{hard-margin SVM}? Why or why not?
        
        Could a \textbf{soft-margin SVM} be used? If yes, justify your answer by describing what the resulting linear classifier would do (i.e., where would its 1D decision boundary likely be, and which points would it misclassify? consider two values of the model complexity hyperparameter $C$: $1$ and $1000$). 
        
        \begin{solutionbox}
        	As we know, the hard-margin SVM will not tolerate any misclassification; therefore, there is no such decision boundary in 1D, hence the hard-margin SVM cannot solve the problem.\\

        \end{solutionbox}
    
    \end{enumerate}

    % --- QUESTION 3 [Valid Kernel Functions] ---
    \item \textbf{(8 points) [Constructing Valid Kernels]} \\
    Let $K_1$ and $K_2$ be a valid kernel functions, with feature mapping $\varphi_1 : \mathbb{R}^d \to \mathbb{R}^{d_1}$ and $\varphi_2 : \mathbb{R}^d \to \mathbb{R}^{d_2}$.
    
    \begin{enumerate}[label=(\roman*)]
        \item \textbf{(2 points)} Show that $K_3 = K_1 + K_2$ is also a valid kernel. Give the feature mapping $\varphi_3$ corresponding to $K_3$ in terms of $\varphi_1$ and $\varphi_2$.
        
        \begin{solutionbox}
        	As $K_1$ and $K_2$ are valid kernel functions with corresponding feature mapping $\varphi_{1}:\mathbb{R}^{d}\to\mathbb{R}^{d_{1}}$ and $\varphi_{2}:\mathbb{R}^{d}\to\mathbb{R}^{d_{2}}$, we can write 
        	\begin{align*}
        		K_{1}&=\innerp{\varphi_{1}(x_{i})}{\varphi_{1}(x_{j})}=\varphi_{1}(x_{i})^{T}\varphi_{1}(x_{j})\\
        		K_{2}&=\innerp{\varphi_{2}(x_{i})}{\varphi_{2}(x_{j})}=\varphi_{2}(x_{i})^{T}\varphi_{2}(x_{j})
         	\end{align*}
        	where $x_i,x_{j}\in\mathbb{R}^{d}$. Now, define
        	\begin{align*}
        		K_{3}&=K_{1}+K_{2}=\innerp{\varphi_{1}(x_{i})}{\varphi_{1}(x_{j})}+\innerp{\varphi_{2}(x_{i})}{\varphi_{2}(x_{j})}\\
        		&=\varphi_{1}(x_{i})^{T}\varphi_{1}(x_{j})+\varphi_{2}(x_{i})^{T}\varphi_{2}(x_{j})\\
        		&=\begin{bmatrix}\varphi_{1}(x_i)^{T} & \varphi_{2}(x_{i}^{T})\end{bmatrix}\begin{bmatrix}\varphi_{1}(x_{j})\\\varphi_{2}(x_{j})\end{bmatrix}\\
        		&=\innerp{\begin{bmatrix}\varphi_{1}(x_{i})\\\varphi_{2}(x_{i})\end{bmatrix}}{\begin{bmatrix}\varphi_{1}(x_{j})\\\varphi_{2}(x_{j})\end{bmatrix}}
        	\end{align*}
        	Hence, $K_{3}$ is a valid kernel corresponding to the feature mapping
        	\begin{align*}
        		\varphi_{3}=\begin{bmatrix}\varphi_{1}\\\varphi_{2}\end{bmatrix}:\mathbb{R}^{d}\to\mathbb{R}^{d_{1}+d_{2}}
        	\end{align*}
        \end{solutionbox}
        
        \item \textbf{(2 points)} Show that $K_4 = K_1 \cdot K_2$ is also a valid kernel. Give the feature mapping $\varphi_4$ corresponding to $K_4$ in terms of $\varphi_1$ and $\varphi_2$.
        
        \begin{solutionbox}
        	As $K_1$ and $K_2$ are valid kernel functions with corresponding feature mapping $\varphi_{1}:\mathbb{R}^{d}\to\mathbb{R}^{d_{1}}$ and $\varphi_{2}:\mathbb{R}^{d}\to\mathbb{R}^{d_{2}}$, we can write 
        	\begin{align*}
        		K_{1}&=\innerp{\varphi_{1}(x_{i})}{\varphi_{1}(x_{j})}=\varphi_{1}(x_{i})^{T}\varphi_{1}(x_{j})\\
        		K_{2}&=\innerp{\varphi_{2}(x_{i})}{\varphi_{2}(x_{j})}=\varphi_{2}(x_{i})^{T}\varphi_{2}(x_{j})
        	\end{align*}
        	where $x_i,x_{j}\in\mathbb{R}^{d}$. Now, define
        	\begin{align*}
        		K_{4}&=K_{1}\cdot K_{2}=\innerp{\varphi_{1}(x_{i})}{\varphi_{1}(x_{j})}\cdot\innerp{\varphi_{2}(x_{i})}{\varphi_{2}(x_{j})}\\
        		&=\innerp{\begin{bmatrix}\varphi_{11}(x_{i})\\\varphi_{12}(x_{i})\\\vdots\\\varphi_{1d_{1}}(x_{i})\end{bmatrix}}{\begin{bmatrix}\varphi_{11}(x_{j})\\\varphi_{12}(x_{j})\\\vdots\\\varphi_{1d_{1}}(x_{j})\end{bmatrix}}\cdot\innerp{\begin{bmatrix}\varphi_{21}(x_{i})\\\varphi_{22}(x_{i})\\\vdots\\\varphi_{2d_{2}}(x_{i})\end{bmatrix}}{\begin{bmatrix}\varphi_{21}(x_{j})\\\varphi_{22}(x_{j})\\\vdots\\\varphi_{2d_{2}}(x_{j})\end{bmatrix}}\\
        		&=\left[\sum_{k=1}^{d_1}\varphi_{1k}(x_{i})\varphi_{1k}(x_{j})\right]\left[\sum_{l=1}^{d_2}\varphi_{2l}(x_{i})\varphi_{2l}(x_{j})\right]\\
        		&=\sum_{l=1}^{d_{2}}\sum_{k=1}^{d_{1}}\varphi_{1k}(x_{i})\varphi_{2l}(x_{i})\varphi_{1k}(x_{j})\varphi_{2l}(x_{j})\\
        		&=\innerp{\begin{bmatrix}\varphi_{11}(x_{i})\varphi_{21}(x_{i})\\\vdots\\\varphi_{1d_{1}}(x_{i})\varphi_{2d_{2}}(x_{i})\end{bmatrix}}{\begin{bmatrix}\varphi_{11}(x_{j})\varphi_{21}(x_{j})\\\vdots\\\varphi_{1d_{1}}(x_{j})\varphi_{2d_{2}}(x_{j})\end{bmatrix}}\\
        		&=\innerp{\varphi_{1}(x_{i})\otimes\varphi_{2}(x_{i})}{\varphi_{1}(x_{j})\otimes\varphi_{2}(x_{j})}
        	\end{align*}

        	Hence, $K_{4}$ is a valid kernel corresponding to the feature mapping
        	\begin{align*}
				\varphi_{4}=\varphi_{1}\otimes\varphi_{2}:\mathbb{R}^{d}\to\mathbb{R}^{d_{1}d_{2}}
			\end{align*}
        	where $\otimes$ is Kronecker product.
        \end{solutionbox}
        
        \item \textbf{(2 points)} Show that $K_5 = f(u) K_1(u,v) f(v)$ is also a valid kernel for any function $f : \mathbb{R}^d \to \mathbb{R}$. Give the feature mapping $\varphi_5$ corresponding to $K_5$ in terms of $\varphi_1$ and $f$.
        
        \begin{solutionbox}
        	As $K_1$ is a valid kernel functions with feature mapping $\varphi_{1}:\mathbb{R}^{d}\to\mathbb{R}^{d_{1}}$, we can write 
        	\begin{align*}
        		K_{1}(u,v)=\varphi_{1}(u)^{T}\varphi_{1}(v)
        	\end{align*}
        	where $u,v\in\mathbb{R}^{d}$. Now define
        	\begin{align*}
        		K_{5}(u,v)&=f(u)K_{1}(u,v)f(v)\\
        		&=f(u)\varphi_{1}(u)^{T}\varphi_{1}(v)f(v)\\
        		&=\innerp{f(u)\varphi_{1}(u)}{f(v)\varphi_{1}(v)}
        	\end{align*}
        	where $f:\mathbb{R}^{d}\to\mathbb{R}$.\\
        	Hence, $K_{5}$ is a valid kernel corresponding to the feature mapping 
        	\begin{align*}
        		\varphi_{5}=\varphi_{1}f : \mathbb{R}^{d}\to\mathbb{R}^{d_{1}}
        	\end{align*} 
        \end{solutionbox}
        
        \item \textbf{(2 points)} Show that a Kernel given by $K(u,v) = \exp(2u^T v)$ is a valid kernel. [Hint: Use the results above on a polynomial expansion of $\exp(t)$.]
        
        \begin{solutionbox}
        	By Taylor series we have
        	\begin{align*}
        		K(u,v) &= \exp(2u^T v) = \sum_{k=0}^{\infty} \frac{(2u^T v)^k}{k!} = \sum_{k=0}^{\infty} \frac{2^k}{k!} (u^T v)^k=\sum_{k=0}^{\infty}K_{k}(u,v)
        	\end{align*}
        	where $K_k(u,v)=\frac{2^k}{k!} (u^T v)^k$.
        	
        	Equivalently we can write
        	\begin{align*}
        		K_k(u,v)=f_{k}(u)K_{k}^{'}(u,v)f_{k}(v)
        	\end{align*}
        	where $f_{k}=\sqrt{\frac{2^{k}}{k!}}$ and $K_{k}^{'}=(u^{T}v)^{k}$.\\
        	Now
        	\begin{align*}
        		K_{k}^{'}(u,v)&=(u^{T}v)^{k}=\innerp{u}{v}^{k}=\innerp{u^{\otimes k}}{v^{\otimes k}}=\innerp{\varphi_{k}^{'}(u)}{\varphi_{k}^{'}(v)}
        	\end{align*}
        	where $\varphi_{k}^{'}(\cdot)=(\cdot)^{\otimes k}$ denotes the $k^{\text{th}}$ tensor power. Hence, $K_{k}^{'}$ is a valid kernel corresponding to the feature map $\varphi_{k}^{'}$. 
        	
        	Moreover, using the result from the previous problem (that $f(u)K_1(u,v)f(v)$ is a valid kernel), we have
        	\begin{align*}
        		K_k(u,v)&=f_k(u) \innerp{\varphi_k'(u)}{\varphi_k'(v)} f_k(v) \\
        		&= \innerp{f_k(u) \varphi_k'(u)}{f_k(v) \varphi_k'(v)}.
        	\end{align*}
        	This shows that each $K_k(u,v)$ is a valid kernel with feature map 
        	\begin{align*}
        		\varphi_k(u) = f_k(u) \varphi_k'(u) = \sqrt{\frac{2^k}{k!}} u^{\otimes k}.
        	\end{align*}
        	
        	Finally,
        	\begin{align*}
        		K(u,v)&=\sum_{k=0}^{\infty} K_{k}(u,v)=\sum_{k=0}^{\infty} \innerp{\varphi_{k}(u)}{\varphi_{k}(v)}=\innerp{\begin{bmatrix}\varphi_{0}(u)\\\varphi_{1}(u)\\\vdots\\\end{bmatrix}}{\begin{bmatrix}\varphi_{0}(v)\\\varphi_{1}(v)\\\vdots\\\end{bmatrix}}
        	\end{align*}
	        Hence, $K$ is a valid kernel corresponding to the feature mapping 
	        \begin{align*}
	        	\varphi(u)=\begin{bmatrix}
	        		\varphi_0(u) \\
	        		\varphi_1(u) \\
	        		\varphi_2(u) \\
	        		\vdots
	        	\end{bmatrix}
	        	=
	        	\begin{bmatrix}
	        		\sqrt{\frac{2^0}{0!}} u^{\otimes 0} \\
	        		\sqrt{\frac{2^1}{1!}} u^{\otimes 1} \\
	        		\sqrt{\frac{2^2}{2!}} u^{\otimes 2} \\
	        		\vdots
	        	\end{bmatrix}
	        \end{align*} 
	        where $u^{\otimes 0}=1$. 
        \end{solutionbox}
        
        \item \textbf{(0 points) (Optional)} Show that a Kernel given by $K(u,v) = \exp(-\|u-v\|^2)$ is a valid kernel. [Hint: Use the last two parts' results.]
        
        \begin{solutionbox}
        	The given kernel
        	\begin{align*}
        		K(u,v)&=e^{-\norm{u-v}{2}^{2}}=e^{-\norm{u}{2}^{2}+2u^{T}v-\norm{v}{2}^{2}}=e^{-\norm{u}{2}^{2}}\cdot e^{2u^{T}v}\cdot e^{-\norm{v}{2}^{2}}.
        	\end{align*}
        	Using the results from previous problems, we can write
        	\begin{align*}
        		K(u,v)=\innerp{f(u)\varphi(u)}{f(v)\varphi(v)}=\innerp{\tilde{\varphi}(u)}{\tilde{\varphi}(v)}
        	\end{align*}
        	where 
        	\begin{align*}
        		f(u)=\exp(-\norm{u}{2}^{2}),\quad\varphi(u)=\begin{bmatrix}
        		\sqrt{\frac{2^0}{0!}} u^{\otimes 0} \\
        		\sqrt{\frac{2^1}{1!}} u^{\otimes 1} \\
        		\sqrt{\frac{2^2}{2!}} u^{\otimes 2} \\
        		\vdots
        	\end{bmatrix}
        \end{align*}
        	Hence, $K$ is a valid kernel corresponding to the feature mapping
        	\begin{align*}
        		\tilde{\varphi}(u)=e^{-\norm{u}{2}^{2}}\begin{bmatrix}
        			\sqrt{\frac{2^0}{0!}} u^{\otimes 0} \\
        			\sqrt{\frac{2^1}{1!}} u^{\otimes 1} \\
        			\sqrt{\frac{2^2}{2!}} u^{\otimes 2} \\
        			\vdots
        		\end{bmatrix}
        	\end{align*}
        \end{solutionbox}
    
    \end{enumerate}

    % --- QUESTION 4 [Primal, Dual, and Complexity of Ridge Regression] ---
    \item \textbf{(8 points) [Primal, Dual, and the Representer Theorem]} \\
    Consider the primal objective function for Ridge Regression in a feature space (with features $\phi(x)$):
    \[ \min_{w \in \mathbb{R}^{d'}} \left(L(w) := \frac{1}{2} ||\Phi w - y||^2 + \frac{1}{2} ||w||^2 \right) \]
    where $\Phi$ is the $n \times d'$ design matrix with rows $\phi(x_i)^T$, $y$ is the $n \times 1$ target vector, $w$ is the $d' \times 1$ weight vector, and $\lambda > 0$ is the regularization parameter.
    
    \begin{enumerate}[label=(\alph*)]
        \item \textbf{(1 point)} Using the Representer Theorem, get the dual version of the given primal problem.
        
        \begin{solutionbox}
        	Let, $u+v \text{ where } u\in \operatorname{null}(\Phi);~v\in\operatorname{col}(\Phi^{T})$\\
        	The objective
        	\begin{align*}
        		\min_{w \in \mathbb{R}^{d'}} &\;\frac{1}{2} \norm{\Phi w - y}{2}^2 + \frac{1}{2} \norm{w}{2}^2 \\
        		&\qquad\qquad\big\Updownarrow\\
        		\min_{\substack{u \in \operatorname{null}(\Phi) \\ v \in \operatorname{col}(\Phi^{T})}}
        		&\;\frac{1}{2}\norm{\Phi(u+v)-y}{2}^{2}
        		+ \frac{1}{2}\norm{u}{2}^{2}
        		+ \frac{1}{2}\norm{v}{2}^{2}\\
        		&\qquad\qquad\big\Updownarrow \\
        		\min_{v \in \operatorname{col}(\Phi^{T})}
        		&\;\frac{1}{2}\norm{\Phi v - y}{2}^{2}
        		+ \frac{1}{2}\norm{v}{2}^{2}\\
        		&\qquad\qquad\big\Updownarrow\\
        		\min_{\alpha \in \mathbb{R}^{n}}\frac{1}{2}&\;\norm{\Phi\Phi^{T}\alpha-y}{2}^{2}+\frac{1}{2}\norm{\Phi^{T}\alpha}{2}^{2}
        	\end{align*}
        	
        	Hence, the dual problem is 
        	\begin{align*}
        		\min_{\alpha \in \mathbb{R}^{n}} R(\alpha) := \frac{1}{2}\norm{\Phi\Phi^{T}\alpha-y}{2}^{2}+\frac{1}{2}\norm{\Phi^{T}\alpha}{2}^{2}
        	\end{align*}
        \end{solutionbox}
        
        \item \textbf{(2 points)} Find the optimal $\hat{\alpha}$ by computing the gradient of the dual objective, $\nabla R(\alpha)$, setting it to zero, and solving for $\alpha$.
        
        \begin{solutionbox}
        	The gradient of $R(\alpha)$
        	\begin{align*}
        		\nabla R(\alpha)&=\Phi\Phi^{T}\left(\Phi\Phi^{T}\alpha-y\right)+\Phi\Phi^{T}\alpha\\
        		&=\Phi\Phi^{T}\left((\Phi\Phi^{T}+\mathbb{I})\alpha-y\right)
        	\end{align*}
        	Now setting $\nabla R(\alpha)$ to zero
        	\begin{align*}
        		&\nabla R(\alpha)=0\\
        		\Rightarrow&\Phi\Phi^{T}\left((\Phi\Phi^{T}+\mathbb{I})\alpha-y\right)=0\\
        		\Rightarrow&\hat{\alpha}=\left(\Phi\Phi^{T}+\mathbb{I}\right)^{-1}y
        	\end{align*}
        \end{solutionbox}
        
        \item \textbf{(1 point)} Show how the learned function $f(x) = \hat{w}^T \phi(x)$ can be evaluated on a new test point $x$ using only the dual solution $\hat{\alpha}$ and kernel function evaluations $K(x_i, x) = \phi(x_i)^T \phi(x)$.
        
        \begin{solutionbox}
        	The learned function can be written as
        	\begin{align*}
        		f(x)&=\hat{w}^{T}\phi(x)\\
        		&=\hat{\alpha}^{T}\Phi\phi(x)=\hat{\alpha}^{T}\begin{bmatrix}
        			\phi(x_1)^{T}\\
        			\vdots\\
        			\phi(x_n)^{T}
        		\end{bmatrix}\phi(x)\\
        		&=\sum_{i=1}^{n}\hat{\alpha}_{i}\phi(x_{i})^{T}\phi(x)=\sum_{i=1}^{n}\hat{\alpha}_{i}K(x_{i},x)
        	\end{align*}
        \end{solutionbox}
    
        \item \textbf{(2 points)} The standard primal solution $\hat{w}_{primal}$ is found by solving $\nabla L(w) = 0$ for $w$. The dual-derived solution is $\hat{w}_{dual} = \Phi^T \hat{\alpha}$.
        
        Argue that both are identical and use it to prove the push-through matrix identity (also related to the Sherman–Morrison–Woodbury identity):\\ $(\Phi^T \Phi + I)^{-1} \Phi^T = \Phi^T (\Phi \Phi^T + I)^{-1}$.

        \begin{solutionbox}
        	The standard primal solution is
        	\begin{align*}
        		\nabla L(w)&=0\\
        		\Rightarrow \hat{w}_{primal}&= \left(\Phi^{T}\Phi+\mathbb{I}\right)^{-1}\Phi^{T}y
        	\end{align*}
        	and the dual-derived solution is
        	\begin{align*}
        		\hat{w}_{dual}&=\Phi^{T}\hat{\alpha}=\Phi^{T}\left(\Phi\Phi^{T}+\mathbb{I}\right)^{-1}y
        	\end{align*}
        	Since these two are equal (because convex objective in $w$) for every $y$ , the linear operators must be same
        	\begin{align*}
        		\left(\Phi^{T}\Phi+\mathbb{I}\right)^{-1}\Phi^{T}=\Phi^{T}\left(\Phi\Phi^{T}+\mathbb{I}\right)^{-1}
        	\end{align*}
        	which is exactly the push-through matrix identity.
        \end{solutionbox}
        
        \item \textbf{(2 points)} Compare the computational complexity for calculating the two $\hat{w}$ solutions from part (d).
        
        Provide a step-by-step breakdown of the time complexity (in asymptotic big-Oh notation) for computing $\hat{w}_{primal}$ and $\hat{w}_{dual}$.
        Based on your analysis, when is the primal solution more efficient to compute, and when is the dual solution more efficient?
    
        \begin{solutionbox}
        	For primal problem
        	\begin{itemize}
        		\item forming $\Phi^T\Phi$ cost : $O(d'^2n)$
        		\item forming $(\Phi^T \Phi +\mathbb{I})^{-1}$ cost : $O(d'^3)$
        		\item forming $\Phi^Ty$ cost : $O(d'n)$
        		\item multiplying $(\Phi^T\Phi+\mathbb{I})^{-1}$ with $\Phi^{T}y$ cost : $O(d'^2)$
        	\end{itemize}
        	the total cost: $O(n d'^2 + d'^3).$\\
        	For dual problem
        	\begin{itemize}
        		\item forming $\Phi\Phi^T$ cost : $O(n^2 d')$
        		\item forming $(\Phi\Phi^T+\mathbb{I})^{-1}$ cost : $O(n^3)$
        		\item multiplying $\Phi^T$ with $(\Phi\Phi^T +\mathbb{I})^{-1}$ cost : $O(n^2 d')$
        		\item multiplying $\Phi^T(\Phi\Phi^T+\mathbb{I})^{-1}$ with $y$ cost : $O(nd')$
        	\end{itemize}
        	the total cost: $O(n^2 d' + n^3)$.\\
        	Hence, when $d^{'}\ll n$(i.e., feature space is smaller then the number of data points) solving the primal problem is computationally efficient (because $O(n d'^2 + d'^3)\ll O(n^2 d' + n^3)$) whereas if $d^{'}\gg n$(i.e., number of data points is smaller then the feature space) solving the dual problem is computationally efficient (because $O(n d'^2 + d'^3)\gg O(n^2 d' + n^3)$ ).
        \end{solutionbox}
    
    \end{enumerate}

    % --- QUESTION 5 [MAXIMIZING THE MARGIN WITH SVMS]---
    \item \textbf{(8 points) [MAXIMIZING THE MARGIN WITH SVMS]} \\
    Consider a simple, linearly separable 2D dataset for a Support Vector Machine (SVM) classifier:
    \begin{itemize}
        \item \textbf{Class +1:} $(2, 3)$, $(3, 3)$
        \item \textbf{Class -1:} $(1, 1)$, $(2, 1)$
    \end{itemize}
    The decision boundary is a hyperplane of the form $w^T x + b = 0$. For a canonical hyperplane, the margin is $2/||w||$, and for all data points $(x_i, y_i)$, we have $y_i(w^T x_i + b) \ge 1$.

    \begin{enumerate}[label=(\alph*)]
        \item \textbf{(3 points)} By inspection or simple geometry, determine the equation of the maximal margin hyperplane that separates these two classes. Identify the weight vector $w$ and the intercept/bias $b$.
        \begin{solutionbox}
        	As we can see from Figure \ref{fig:SVD_boundary_a} the decision boundary which maximize the margin is 
        	\begin{align*}
        		&x_{2}=2\Rightarrow \begin{bmatrix}0 & 1\end{bmatrix}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}-2=0
        	\end{align*}
        	writing it in standard hyperplane form $w^{T}x+b=0$, we have $w=\begin{bmatrix}0\\1\end{bmatrix}$ and $b=-2$
        	\begin{center}
        		\includegraphics[width=.8\textwidth]{figure_5a.pdf}
        		\captionof{figure}{Data points with decision boundary}
        		\label{fig:SVD_boundary_a}
        	\end{center}
    \end{solutionbox} 
    \item \textbf{(3 points)} Identify the support vectors from the dataset. How do these points satisfy the condition $y_i(w^T x_i + b) = 1$?
    \begin{solutionbox}
    	For class $+1$ the support vectors are $(2,3)$ and $(3,3)$.\\
    	For class $-1$ the support vectors are $(1,1)$ and $(2,1)$.\\
    	Consider the datapoints $(2,3)$ and $(3,3)$. For these point $y=+1$, so
    	\begin{align*}
    		y_{i}(w^{T}x_{i}+b)=+1\left(\begin{bmatrix}
    			0 & 1
    		\end{bmatrix}x_{i}-2\right)=1
    	\end{align*}    	
    	similarly for the datapoints $(1,1)$ and $(2,1)$ $y=-1$ so, we have
    	\begin{align*}
    		y_{i}(w^{T}x_{i}+b)=-1\left(\begin{bmatrix}0 & 1\end{bmatrix}x_{i}-2\right)=1
    	\end{align*}
    \end{solutionbox} 
        \item \textbf{(2 points)} Suppose we remove the point $(1, 1)$ from the dataset. Would the decision boundary change? Now, suppose we instead remove the point $(2, 3)$. Would the decision boundary change in this case? Explain your reasoning.

    \begin{solutionbox}
    	If we remove the point $(1,1)$, the decision boundary remains unchanged. Since removing the support vector does not increase the distance between the class hulls, the previous optimal hyperplane remains optimal.\\
    	Now, if we remove $(2,3)$, the positive class is only at $(3,3)$, which will be a support vector, and the nearest point from the negative hull(i.e., convex combination of $(1,1)$ and $(2,1)$) is $(2,1)$. Moreover, the maximum margin hyperplane is the perpendicular bisector of the segment joining the points $(2,1)$ and $(3,3)$. Therefore, the perpendicular bisector has a midpoint of $(2.5, 2)$ and a normal vector of $(1, 2)$. Finally, the equation is
    	\begin{align*}
    		(x_{1}-2.5)+2(x_{2}-2)&=0\Rightarrow x_{1} +2 x_{2} -6.5=0\\
    		\Rightarrow \begin{bmatrix}\frac{2}{5}&\frac{4}{5}\end{bmatrix}x -2.6&=0\Rightarrow w^{T}x+b = 0 
    	\end{align*}
    	where $w=\begin{bmatrix}0.4\\0.8\end{bmatrix}$ and $b=-2.6.$\\
    	Figure \ref{fig:SVD_boundary_b} shows the new decision boundary after removing the datapoint $(2,3)$.
    	\begin{center}
    		\includegraphics[width=.8\textwidth]{figure_5c.pdf}
    		\captionof{figure}{New decision boundary after removing datapoint $(2,3)$}
    		\label{fig:SVD_boundary_b}
    	\end{center}
    \end{solutionbox} 
    \end{enumerate}
    % --- QUESTION 6 [GUESS-TIMATING BIAS AND VARIANCE]---
    \item \textbf{(15 points) [GUESS-TIMATING BIAS AND VARIANCE]} \\
    You are given a dataset consisting of 100 datapoints in \href{https://drive.google.com/drive/folders/1zW42_WnTQDCYy22ZbLPQmwVjFZyX3ZrP?usp=sharing}{this folder}. You have to fit a polynomial ridge regression model to this data.

    A model's error can be decomposed into bias, variance, and noise. A ``learning curve'' provides an opportunity to determine the bias and variance of machine learning models, and to identify models that suffer from high bias (underfitting) or high variance (overfitting). The ``learning curve'' typically shows the training error and validation/testing error on the y-axis and the model complexity on the x-axis.

    \begin{enumerate}[label=(\alph*)]
        \item \textbf{(2 points)} Read the last Section (Section 4) on "Bias and Variance in practice" in this \href{https://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf}{document}, and summarize briefly how you will heuristically find whether your model suffers from (i) high bias, or (ii) high variance, using only the train and validation/test errors of the model.
        \begin{solutionbox}
        	As mentioned in the \href{https://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf}{document}  (i) high bias leads to under fitting and (ii) high variance leads to over fitting on the training data. Therefore heuristically I can say if the model performs poorly(i.e., high error) on the training as well as validation/test dataset it has high bias(under fit) and if the model performs good(i.e., low error) in training dataset but suffers in test/validation dataset then it has high variance(over fit).     
        \end{solutionbox}
        
        \item \textbf{(4 points)} Start with the code for polynomial regression and add quadratic regularization functionality to the code. That is, your code should do polynomial regression with quadratic regularization that takes degree $d$ and regularization parameter $\lambda$ as input. 
        \red{Do not use any inbuilt functions from Python packages (except for plotting functions, functions to compute polynomial features for each data point, and functions for (pseudo)inverse of matrices).}
        \begin{solutionbox}
        	Written code and attached below
        \end{solutionbox}
        
        \item \textbf{(4 points)} Run your code on the provided dataset for degree $d=24$ and each $\lambda$ in the set: 
        \[\{10^{-15}, 10^{-9}, 10^{-6}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^{1}, 10^{2}, 10^{3}, 10^{6}, 10^{9}, 10^{15}\}\]
         \begin{enumerate}
             \item[i.] Perform 5-fold cross-validation on the 100 data points (20 data points in each fold). For each validation fold, compute both training (4-fold-based) and validation (1-fold-based) errors using the mean squared error measure. 
             \item[ii.] Calculate the average training and validation errors across the 5 folds.
         \end{enumerate}
         \begin{solutionbox}
         	The average training and validation errors across the $5$ folds are as follows
         	\begin{center}
         		\begin{tabular}{|c|c|c|}\hline
         			$\lambda$ & Training error & Validation error\\\hline
         			$10^{-15}$ &$ 10.4015$ & $62.6490$\\\hline
         			$10^{-9}$ &$ 0.9076$ & $1.3552$\\\hline
         			$10^{-6}$ &$ 0.9284$ & $1.2113$\\\hline
         			$10^{-3}$ &$ 1.0180$ & $1.2334$\\\hline
         			$10^{-2}$ &$1.1219$ & $1.2577$\\\hline
         			$10^{-1}$ &$ 1.3129$ & $1.4736$\\\hline
         			$1$ &$ 1.6767$ & $1.8537$\\\hline
         			$10$ &$ 2.4839$ & $2.7989$\\\hline
         			$10^{2}$ &$ 3.6237$ & $4.0391$\\\hline
         			$10^{3}$ &$ 4.3157$ & $4.3851$\\\hline
         			$10^{6}$ &$ 4.4512$ & $4.5091$\\\hline
         			$10^{9}$ &$ 4.4440$ & $4.5753$\\\hline
         			$10^{15}$ &$ 4.4542$ & $4.4830$\\\hline
         		\end{tabular}
         	\end{center}
         \end{solutionbox}
        

        \item \textbf{(3 points)} Construct a learning curve by plotting the average training and validation errors against the model complexity ($\log_{10} \lambda$). Based on this learning curve, identify the (i) model with the highest bias, (ii) model with the highest variance?, and (iii) the model that will work best on some unseen data.
        \begin{solutionbox}
        	From the Figure \ref{fig:learning_curve} we can see
        	\begin{enumerate}[label=(\roman*)]
        		\item \textbf{Model with highest bias:} $10^{15}$
        		\item \textbf{Model with highest variance:} $10^{-15}$
        		\item \textbf{Model that will work best on some unseen data:} $10^{-6}$
        	\end{enumerate}
        	\begin{center}
        		\includegraphics[width=\textwidth]{plots/learning_curve_polynomial_regression.pdf}
        		\captionof{figure}{Learning Curve}
        		\label{fig:learning_curve}
        	\end{center}
        \end{solutionbox}

        \item \textbf{(2 points)} Plot the fitted curve to the given data ($\hat{y}$ against $x$ curve) for the three models reported in part (d) and superimposed with the training and validation data points for any one-fold. 
        
        \begin{solutionbox}
        	Figure \ref{fig:Fitted_polynominal} shows the fitted curve to the given data for models reported in part (d) with one fold training and validation data points.
        	\begin{center}
        		\includegraphics[width=\textwidth]{plots/fitted_polynomial_curves_one_fold.pdf}
        		\captionof{figure}{Fitted Polynomial (degree $24$) curves}
        		\label{fig:Fitted_polynominal}
        	\end{center}
        \end{solutionbox}
        \end{enumerate}
        
\end{enumerate}

    % --- SELF DECLARATION ---
    \vspace{10mm}
    \hrule
    \begin{center}
        \large\textbf{Self Declaration}
    \end{center}
    \vspace{3mm}
    I, \underline{RITABRATA MANDAL} , swear on my honour that I have prepared and written the answers for this assignment and associated code by myself and have not copied it from the internet, any LLM's output, or other students.
    \vspace{5mm}
    \hrule
    \vspace{10mm}

% --- DOCUMENT END ---
\end{document}