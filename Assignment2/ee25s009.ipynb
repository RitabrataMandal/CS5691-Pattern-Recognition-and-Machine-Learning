{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342ziGnewCr7"
      },
      "source": [
        "# General Instructions to students:\n",
        "\n",
        "1. There are 4 types of cells in this notebook. The cell type will be indicated within the cell.\n",
        "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
        "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
        "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
        "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
        "    \n",
        "2. You are not allowed to insert new cells in the submitted notebook.\n",
        "\n",
        "3. You are not allowed to import any extra packages, unless needed.\n",
        "\n",
        "4. The code is to be written in Python 3.x syntax. Latest versions of other packages maybe assumed.\n",
        "\n",
        "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/printed.\n",
        "\n",
        "6. If TextWrite cells ask you to give accuracy/error/other numbers, you can print them on the code cells, but remove the print statements before submitting.\n",
        "\n",
        "7. Any runtime failures on the submitted notebook will get zero marks.\n",
        "\n",
        "8. All code must be written by you. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.\n",
        "\n",
        "10. All plots must be labelled properly, the labels/legends should be readable, all tables must have rows and columns named properly.\n",
        "\n",
        "11. Change the name of file with your roll no. For example cs15d203.ipynb (for notebook) and cs15d203.py (for plain python script)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "We3kiiDJ8eV2"
      },
      "outputs": [],
      "source": [
        "# Cell type : CodeRead\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0s0y4Y8wbTC"
      },
      "source": [
        "\n",
        "**Cell type : TextRead**\n",
        "\n",
        "Question 6: Coding Life in Lower Dimensions\n",
        "\n",
        "You are provided with a [dataset](https://drive.google.com/file/d/1wOp8K9BS8Ncmjz7aP8RdqukqkGNSwK4i/view?usp=drive_link) of 1797 images - each image is 8x8 pixels and provided as a feature vector of length 64. You will try your hands at transforming this dataset to a lower-dimensional space using PCA.\n",
        "\n",
        "6a) Code up and run the PCA algorithm on the given dataset. Plot the cumulative percentage variance explained by the principal components. Report the number of principal components that contribute to 90% of the variance in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hTbuarfwjmz"
      },
      "outputs": [],
      "source": [
        "# Cell type : CodeWrite\n",
        "# Write the function for PCA here.\n",
        "def computePCA(Data, M=None):\n",
        "    \"\"\" Compute the principal components of the dataset of dimension N X D (n_samples x n_features)\n",
        "    Feel free to modify the function arguments and outputs to your needs, but we will provide some recommendations here.\n",
        "\n",
        "    Arguments:\n",
        "    Data: Dataset\n",
        "    M: If not provided, return all the principal components (PCs). If provided, return only the top M PCs.\n",
        "\n",
        "    Returns:\n",
        "    transformed_D: transformed (reconstructed) data using top M PCs.\n",
        "    components:\n",
        "    explained_variation: of each component\n",
        "    numTopPCs: if M=None, provides minimum number of principal components that contribute to at least 90% of variance in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    #your code here\n",
        "\n",
        "    return transformed_D, components, explained_variation, numTopPCs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_P8cXAFwsol"
      },
      "source": [
        "6b)  Perform reconstruction of data using the dimensionality-reduced data considering the number of dimensions [2,4,8,16]. Report the Mean Square Error (MSE) between the original data and reconstructed data, and interpret the optimal dimensions $\\hat{d}$ based on the MSE values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8QIbVLkw3Pz"
      },
      "outputs": [],
      "source": [
        "# Cell type : CodeWrite\n",
        "# Write the code for data reconstruction, run the algorithm for selecting the number of dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP9rXnAkq3qz"
      },
      "source": [
        " 6c) Let's now apply the same code that you've written above to analyze images to understand text. Large language models (LLMs) typically analyze text by representing words as vectors, also known as embeddings. You are provided with 768-dimensional embeddings (extracted from a LLM called BERT) of 10 words in the [folder](https://drive.google.com/drive/folders/1yshmnkWoALf2ZwqgZqWwXJk2W5_XuCLG?usp=sharing).\n",
        "\n",
        " Apply your PCA code with $M=2$ principal components on these embeddings to visualize the 10 words in 2-D. Report how much percentage of variation is captured by these two principal components. What does this visualization tell you about the embeddings of related vs.~unrelated words?\n",
        "\n",
        " (Note: If applying the same code to your data matrix takes a long time, then consider applying it to the transpose of the data matrix and use the output PCs to derive the PCs of the original matrix as in Q3 of the Assignment.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It7j4pqOrwPR"
      },
      "outputs": [],
      "source": [
        "# Cell type : CodeWrite\n",
        "# Write your code here as instructed.\n",
        "# (Use the function written previously)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"data/Data.npz\")\n",
        "lst = data.files\n",
        "# print(lst)  # see what keys are inside the file\n",
        "for item in lst:\n",
        "    # print(item)\n",
        "    print(data[item].shape)  # print the shape of each array\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
